{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Specialization in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3. Logistic Regression as Neural Network\n",
    "    - Logistic Regression\n",
    "    - Binary Classification\n",
    "    - Cost Function\n",
    "    - Gradient Descent\n",
    "    - Derivative\n",
    "- 4. Shallow Neural Network\n",
    "    - Computing NN outputs\n",
    "    - Activation Function and Derivatives\n",
    "    - Example with Keras gradient descent\n",
    "- 5. Deep Neural Network\n",
    "    - Deep L-layer NN\n",
    "    - Building blocks\n",
    "    - Forward and Backward propagation\n",
    "    - Parameter vs Hyperparameters\n",
    "    - Example with Keras\n",
    "- 6. Housing-Price problem with NN\n",
    "- 7. An overview of Cat-Dog Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a learning algorithm used in a supervised learning problem when the output ùë¶ are\n",
    "all either zero or one. The goal of logistic regression is to minimize the error between its predictions and\n",
    "training data.\n",
    "\n",
    "Example: Cat vs No - cat\n",
    "    \n",
    "Given an image which is represented by a feature vector ùë•, the algorithm will evaluate the probability of a cat\n",
    "being in that image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ùê∫ùëñùë£ùëíùëõ ùë• , ùë¶ÃÇ = ùëÉ(ùë¶ = 1|ùë•), where 0 ‚â§ ùë¶ÃÇ ‚â§ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters used in Logistic regression are:\n",
    "- The input features vector: ùë• ‚àà ‚Ñù<sup>ùëõùë•</sup>, where ùëõùë• is the number of features\n",
    "- The training label: ùë¶ ‚àà 0,1\n",
    "- The weights: ùë§ ‚àà ‚Ñù<sup>ùëõùë•</sup>, where ùëõùë• is the number of features\n",
    "- The threshold: ùëè ‚àà ‚Ñù\n",
    "- The output: ùë¶ÃÇ = sigmoid(z)\n",
    "- Sigmoid function: s = ùúé(ùë§<sup>ùëá</sup>.ùë• + ùëè) = ùúé(ùëß)= 1/(1 + e<sup>(-z)</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/logistic.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classsification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a binary classification problem, the result is a discrete value output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to train a classifier that the input is an image represented by a feature vector, ùë•, and predicts\n",
    "whether the corresponding label ùë¶ is 1 or 0. In this case, whether this is a cat image (1) or a non-cat image\n",
    "(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/binary.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is store in the computer in three separate matrices corresponding to the Red, Green, and Blue\n",
    "color channels of the image. The three matrices have the same size as the image, for example, the\n",
    "resolution of the cat image is 64 pixels X 64 pixels, the three matrices (RGB) are 64 X 64 each.\n",
    "\n",
    "\n",
    "The value in a cell represents the pixel intensity which will be used to create a feature vector of n-dimension. In pattern recognition and machine learning, a feature vector represents an object, in this\n",
    "case, a cat or no cat.\n",
    "\n",
    "\n",
    "To create a feature vector, ùë•, the pixel intensity values will be ‚Äúunrolled‚Äù or ‚Äúreshaped‚Äù for each color. The\n",
    "dimension of the input feature vector ùë• is ùëõùë• = 64 * 64 * 3 = 12,288."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/binary_input.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost and Loss Function\n",
    "\n",
    "There are several ways to learn the parameters of a Machine Learning model. <br>\n",
    "We will focus on the approach that best illustrates statistical learning; minimising a cost function.\n",
    "\n",
    "cost function‚Ää‚Äî‚Ääit helps the learner to correct / change behaviour to minimize mistakes.<br>\n",
    "\n",
    "Put simply, a cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Loss and Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/loss.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function\n",
    "\n",
    "import numpy as np # this means you can access numpy functions by writing np.function() instead of numpy.function()\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (‚âà 1 line of code)\n",
    "    s = 1./(1.+np.exp(-1.*x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the cost function: Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that models learn by minimizing a cost function, you may naturally wonder how the cost function is minimized‚Ää‚Äî‚Ääenter gradient descent. Gradient descent is an efficient optimization algorithm that attempts to find a local or global minima of a function.  \n",
    "\n",
    "It can be simply called as Derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent enables a model to learn the gradient or direction that the model should take in order to reduce errors (differences between actual y and predicted y). \n",
    "\n",
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/gradient.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Gradient\n",
    "\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$ \n",
    "\n",
    "You often code this function in two steps:\n",
    "\n",
    "Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.\n",
    "Compute $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (‚âà 2 lines of code)\n",
    "    s = sigmoid(x)\n",
    "    ds = s*(1-s)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "\n",
    "Logistic activation function in deep learning models, the activation function of a node defines the output of that node, or \"neuron,\" given an input or set of inputs. This output is then used as input for the next node and so on until a desired solution to the original problem is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Function\n",
    "\n",
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/softmax.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (n, m).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (‚âà 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp/x_sum\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and Backward propagation\n",
    "In neural networks, you forward propagate to get the output and compare it with the real value to get the error.\n",
    "\n",
    "Now, to minimize the error, you propagate backwards by finding the derivative of error with respect to each weight and then subtracting this value from the weight value. (For more read [this](https://www.quora.com/What-is-the-difference-between-back-propagation-and-forward-propagation))\n",
    "\n",
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/neural-network_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a powerful learning algorithm inspired by how the brain works.\n",
    "\n",
    "#### Example ‚Äì single neural network\n",
    "Given data about the size of houses on the real estate market and you want to fit a function that will\n",
    "predict their price. The function tries to solve the relationship between the size and the price of houses.\n",
    "\n",
    "It is a linear regression problem because the price as a function of size is a continuous\n",
    "output.\n",
    "We know the prices can never be negative so we are creating a function called Rectified Linear Unit (ReLU)\n",
    "which starts at zero.\n",
    "\n",
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/house1.PNG)\n",
    "\n",
    "- The input is the size of the house (x)\n",
    "- The output is the price (y)\n",
    "- The training data (x, y) is plotted and the model predicts the relationship or the line between them.\n",
    "\n",
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/singleneuron.PNG)\n",
    "\n",
    "Here the ‚Äúneuron‚Äù implements the function ReLU (blue line)\n",
    "\n",
    "# Deep Neural Network\n",
    "\n",
    "#### Example ‚Äì Multiple neural network\n",
    "\n",
    "But in reality the price of a house can be affected by other features such as \n",
    "\n",
    "- size \n",
    "- number of bedrooms\n",
    "- zip code \n",
    "- wealth. \n",
    "\n",
    "The role of the neural network is to predicted the price (red cross) and it will automatically generate the\n",
    "hidden units. We only need to give the inputs x and the output y. \n",
    "\n",
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/house2.PNG)\n",
    "\n",
    "\n",
    "## Difference \n",
    "\n",
    "![](https://raw.githubusercontent.com/Swati707/Machine-Learning-Lab-STC/master/Lab%20Day%202/shallow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting house prices example with Keras\n",
    "### Importing Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Boston Housing Price dataset\n",
    "\n",
    "\n",
    "We will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the \n",
    "suburb at the time, such as the crime rate, the local property tax rate, etc.\n",
    "\n",
    "The dataset we will be using has another interesting thing: it has very few data points, only 506 in \n",
    "total, split between 404 training samples and 102 test samples, and each \"feature\" in the input data (e.g. the crime rate is a feature) has \n",
    "a different scale. For instance some values are proportions, which take a values between 0 and 1, others take values between 1 and 12, \n",
    "others between 0 and 100...\n",
    "\n",
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
       "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
       "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
       "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
       "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
       "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
       "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
       "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
       "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
       "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
       "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
       "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
       "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
       "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
       "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
       "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
       "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
       "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
       "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
       "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
       "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
       "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
       "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
       "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
       "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
       "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
       "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
       "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
       "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
       "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
       "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
       "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
       "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
       "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
       "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
       "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
       "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "\n",
    "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to \n",
    "automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal \n",
    "with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we \n",
    "will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a \n",
    "unit standard deviation. This is easily done in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "\n",
    "Because so few samples are available, we will be using a very small network with two \n",
    "hidden layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using \n",
    "a small network is one way to mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    # Because we will need to instantiate\n",
    "    # the same model multiple times,\n",
    "    # we use a function to construct it.\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363 samples, validate on 41 samples\n",
      "Epoch 1/8\n",
      "363/363 [==============================] - 3s 8ms/step - loss: 166.6200 - mean_absolute_error: 8.8905 - val_loss: 13.0517 - val_mean_absolute_error: 3.0867\n",
      "Epoch 2/8\n",
      "363/363 [==============================] - 3s 9ms/step - loss: 23.4310 - mean_absolute_error: 3.0840 - val_loss: 9.3577 - val_mean_absolute_error: 2.3669\n",
      "Epoch 3/8\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 18.8193 - mean_absolute_error: 2.7602 - val_loss: 9.3822 - val_mean_absolute_error: 2.4701\n",
      "Epoch 4/8\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 15.7043 - mean_absolute_error: 2.6334 - val_loss: 8.0572 - val_mean_absolute_error: 2.3145\n",
      "Epoch 5/8\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 14.8218 - mean_absolute_error: 2.5479 - val_loss: 9.3294 - val_mean_absolute_error: 2.5846\n",
      "Epoch 6/8\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 13.2216 - mean_absolute_error: 2.4393 - val_loss: 7.7387 - val_mean_absolute_error: 2.3614\n",
      "Epoch 7/8\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 12.4759 - mean_absolute_error: 2.3421 - val_loss: 7.4302 - val_mean_absolute_error: 2.1426\n",
      "Epoch 8/8\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 12.1029 - mean_absolute_error: 2.3153 - val_loss: 6.6676 - val_mean_absolute_error: 1.9978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dea2b704a8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, train_targets, validation_split= 0.1,\n",
    "                        epochs=8, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict( test_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output:\n",
      "[[ 7.803312]\n",
      " [18.738247]\n",
      " [21.229338]\n",
      " [31.537516]\n",
      " [25.278505]\n",
      " [19.451117]\n",
      " [26.931568]\n",
      " [22.709377]\n",
      " [20.327257]\n",
      " [21.936008]]\n",
      "\n",
      "Ground Truth:\n",
      "[[ 7.2]\n",
      " [18.8]\n",
      " [19. ]\n",
      " [27. ]\n",
      " [22.2]\n",
      " [24.5]\n",
      " [31.2]\n",
      " [22.9]\n",
      " [20.5]\n",
      " [23.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output:\")\n",
    "print(predict)\n",
    "print()\n",
    "print(\"Ground Truth:\")\n",
    "print(test_targets[:10].reshape(10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
